# Attention is all you need

I've already implemented attention before, but I always forget the details, so let me implement it again.

## Implementations

TODO

## Papers

- https://arxiv.org/pdf/1706.03762 

## Notes
- RNNs can work, but are slow due to necessary sequential computations (and have other issues regarding gradients)
- 
