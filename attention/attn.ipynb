{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The (B, N) is the input and the output targets will also be (B, N) but shift to the right. So the 0th entry predicts the next, the 1st predicts the 2nd, and so on.\n",
    "\n",
    "- We start with (B, N) where b is the batch and N is the max number of tokens we use to predict the next. These tokens are integers here now.\n",
    "- We index into the token embeddings such that for each of the tokens in our past history, to get (B, N, T) where T is the embedding of each token now\n",
    "- To encode positions, we can take a (N, T) embeddings and based on the positions add to (B, N, T) matrix\n",
    "- Then, we take the (B, N, T) and run it through self attention (this notebook aims to show)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give an example of how that input to output tokens would be constructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'H' used to predict the next 'e'\n",
      "'He' used to predict the next 'l'\n",
      "'Hel' used to predict the next 'l'\n",
      "'Hell' used to predict the next 'o'\n",
      "'Hello' used to predict the next ' '\n",
      "'Hello ' used to predict the next 't'\n",
      "'Hello t' used to predict the next 'h'\n",
      "'Hello th' used to predict the next 'e'\n",
      "'Hello the' used to predict the next 'r'\n",
      "'Hello ther' used to predict the next 'e'\n",
      "'Hello there' used to predict the next '!'\n"
     ]
    }
   ],
   "source": [
    "input_str = \"Hello there!\"\n",
    "inputs = range(0, len(input_str)-1)\n",
    "outputs = range(1, len(input_str))\n",
    "\n",
    "for i, j in zip(inputs, outputs):\n",
    "\tprint(f\"'{input_str[:i+1]}' used to predict the next '{input_str[j]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So despite there being in total 10 characters in 'Hello There!' there were in total 11 ways we can take a value string and predict the next character.\n",
    "\n",
    "In other words, during training, when I slice a given string, it will be one less than the context size that I actually want. So account for that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1258, -1.1524],\n",
       "         [-0.2506, -0.4339],\n",
       "         [ 0.8487,  0.6920],\n",
       "         [-0.3160, -2.1152],\n",
       "         [ 0.3223, -1.2633],\n",
       "         [ 0.3500,  0.3081],\n",
       "         [ 0.1198,  1.2377],\n",
       "         [ 1.1168, -0.2473]]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "str_slice_size = 7\n",
    "B = 1\n",
    "T = 2\n",
    "N = str_slice_size+1\n",
    "x = torch.randn((B, N, T))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One strategy to model the dependencies between the embedding is to simply sum them up, then hope that information can be used to predict the next token. For example, we could predict the 5th token, but summing up the previous 4s info and somehow using that info later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prevous 4 tensor([[[-1.1258, -1.1524],\n",
      "         [-0.2506, -0.4339],\n",
      "         [ 0.8487,  0.6920],\n",
      "         [-0.3160, -2.1152]]])\n",
      "summed previous 4 tensor([[[-0.2109, -0.7524]]])\n"
     ]
    }
   ],
   "source": [
    "prev_4 = x[:, :4, :]\n",
    "print(\"prevous 4\", prev_4)\n",
    "print(\"summed previous 4\", prev_4.mean(1, keepdim=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to think about this, is that every value was weighed the same and we did a weighted sum where all the previous values are equally important to predict the next. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2109, -0.7524]]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_4_weighted = torch.ones((B, 4, 1))/4\n",
    "torch.bmm(prev_4_weighted.transpose(1,2), prev_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I can do this all at once for all the various weighted sums pretty easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "         [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "         [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]]])\n",
      "torch.Size([1, 8, 8])\n",
      "torch.Size([1, 8, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1258, -1.1524],\n",
       "         [-0.6882, -0.7931],\n",
       "         [-0.1759, -0.2981],\n",
       "         [-0.2109, -0.7524],\n",
       "         [-0.1043, -0.8546],\n",
       "         [-0.0286, -0.6608],\n",
       "         [-0.0074, -0.3896],\n",
       "         [ 0.1331, -0.3718]]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.tril(torch.ones((B, N, N)))\n",
    "mask /= mask.sum(-1, keepdim=True)\n",
    "print(mask)\n",
    "print(mask.shape)\n",
    "print(x.shape)\n",
    "all = torch.bmm(\n",
    "\tmask, x\n",
    ")\n",
    "all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# just to verify indeed we get all the same averages here!\n",
    "for i in range(0, N):\n",
    "\tprev = x[:, :i+1, :]\n",
    "\tprint(torch.allclose(prev.mean(1), all[:, i, :]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key insight into attention is that the weight values in that mask are not constant, but learned. So we can learn to model which tokens we need to pay attention to in order to predict the next token accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's compute that weight matrix on the fly. \n",
    "\n",
    "The Attention is all you need paper does \n",
    "\n",
    "$$\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "So the softmax portion is what computes the weighted sum basically and the V is essentially the x (although projected).\n",
    "\n",
    "But to make sure that none of the information is accessed before the token is predicted, we causal mask at $QK^T$ which will then produce a matrix that has 0s in the upper triangular, so when we multiply by $V$ we are all good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k = 10\n",
    "Q = torch.randn((B, N, d_k))\n",
    "K = torch.randn((B, N, d_k))\n",
    "V = torch.randn((B, N, d_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.4061,  1.6575, -0.5871, -0.4502,  0.4991, -1.4148, -0.6164,\n",
       "           0.5815],\n",
       "         [-0.1199,  0.9972, -0.7784,  0.5000,  0.9681,  2.2110, -0.6460,\n",
       "           2.5752],\n",
       "         [-0.8138,  0.2423,  0.2972,  0.5500,  0.5481, -0.3925,  0.3161,\n",
       "          -1.8521],\n",
       "         [-1.2321, -0.4832,  0.1016, -0.2666, -0.4511, -1.4859,  0.4228,\n",
       "           0.3468],\n",
       "         [ 0.4941,  0.3841,  1.7922,  0.5413, -0.8161, -1.0426, -0.4646,\n",
       "          -2.1853],\n",
       "         [ 1.3196, -0.5372, -0.0496, -1.1341, -0.2754,  0.6426, -0.2624,\n",
       "          -1.8283],\n",
       "         [ 1.3901,  0.4294, -1.0545, -0.2860,  0.3455,  2.4669, -0.5157,\n",
       "           2.4567],\n",
       "         [ 0.2245, -2.2039,  0.6325,  0.3728, -0.2734, -2.2012,  1.3488,\n",
       "           0.1603]]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QKT = torch.bmm(Q, K.transpose(1,2)) * (d_k**-.5)\n",
    "QKT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax_mask(QKT, mask):\n",
    "\treturn QKT.masked_fill(mask, float(\"-inf\"))\n",
    "\n",
    "def softmax_mask2(QKT, neg_infs):\n",
    "\treturn neg_infs + QKT\n",
    "\n",
    "mask = torch.triu(torch.ones((N, N)), diagonal=1).bool()\n",
    "neg_infs = torch.triu(torch.full((N, N), float(\"-inf\")), diagonal=1)\n",
    "torch.allclose(softmax_mask(QKT, mask), softmax_mask2(QKT, neg_infs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "QKT_masked = softmax_mask(QKT, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2465, 0.7535, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1447, 0.4159, 0.4394, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1049, 0.2218, 0.3980, 0.2754, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1454, 0.1303, 0.5326, 0.1525, 0.0392, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4530, 0.0707, 0.1152, 0.0389, 0.0919, 0.2302, 0.0000, 0.0000],\n",
       "         [0.1964, 0.0751, 0.0170, 0.0367, 0.0691, 0.5764, 0.0292, 0.0000],\n",
       "         [0.1181, 0.0104, 0.1777, 0.1370, 0.0718, 0.0104, 0.3637, 0.1108]]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.softmax(QKT_masked, dim=-1)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.sum(-1) # works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I can just use my scaled value before on V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0118,  0.9797, -1.0661,  1.7720, -0.2793, -0.2769,  0.7489,\n",
       "          -0.6435, -0.9518,  0.2715],\n",
       "         [ 0.5031,  1.6354,  0.6345, -0.0076,  0.6580, -1.2055,  1.8842,\n",
       "           0.7672, -0.6004,  0.4087],\n",
       "         [ 0.8715,  1.1018,  0.1156, -0.0707,  0.4820, -0.6196,  1.2045,\n",
       "           0.0868, -0.2388,  0.1161],\n",
       "         [ 1.1207,  0.6184, -0.3822, -0.0261,  0.7907, -0.6119,  1.6589,\n",
       "          -0.2516, -0.2471,  0.5078],\n",
       "         [ 1.0384,  0.5649, -0.4053,  0.1306,  0.5143, -0.2784,  1.0980,\n",
       "          -0.3669, -0.1385,  0.1659],\n",
       "         [ 0.1054,  0.7243, -0.2848,  0.8400,  0.1375, -0.1596,  0.3091,\n",
       "          -0.0489, -0.6927,  0.0544],\n",
       "         [-0.2171,  0.6273,  0.2146,  0.2448,  0.2760, -0.2169, -0.4110,\n",
       "           0.3250, -0.8813,  0.0926],\n",
       "         [ 0.6966, -0.3517,  0.2894, -0.8149,  0.1164, -0.2958,  0.3576,\n",
       "          -0.5450, -0.2323,  0.6254]]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = torch.bmm(\n",
    "\tweights,\n",
    "\tV\n",
    ")\n",
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0118,  0.9797, -1.0661,  1.7720, -0.2793, -0.2769,  0.7489,\n",
       "          -0.6435, -0.9518,  0.2715],\n",
       "         [ 0.5031,  1.6354,  0.6345, -0.0076,  0.6580, -1.2055,  1.8842,\n",
       "           0.7672, -0.6004,  0.4087],\n",
       "         [ 0.8715,  1.1018,  0.1156, -0.0707,  0.4820, -0.6196,  1.2045,\n",
       "           0.0868, -0.2388,  0.1161],\n",
       "         [ 1.1207,  0.6184, -0.3822, -0.0261,  0.7907, -0.6119,  1.6589,\n",
       "          -0.2516, -0.2471,  0.5078],\n",
       "         [ 1.0384,  0.5649, -0.4053,  0.1306,  0.5143, -0.2784,  1.0980,\n",
       "          -0.3669, -0.1385,  0.1659],\n",
       "         [ 0.1054,  0.7243, -0.2848,  0.8400,  0.1375, -0.1596,  0.3091,\n",
       "          -0.0489, -0.6927,  0.0544],\n",
       "         [-0.2171,  0.6273,  0.2146,  0.2448,  0.2760, -0.2169, -0.4110,\n",
       "           0.3250, -0.8813,  0.0926],\n",
       "         [ 0.6966, -0.3517,  0.2894, -0.8149,  0.1164, -0.2958,  0.3576,\n",
       "          -0.5450, -0.2323,  0.6254]]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def attn(Q, K, V, mask):\n",
    "\tQKT = torch.bmm(Q, K.transpose(1,2)) * (d_k**-.5)\n",
    "\tmasked_QKT = QKT.masked_fill(mask, float(\"-inf\"))\n",
    "\tweights = torch.softmax(masked_QKT, dim=-1)\n",
    "\treturn torch.bmm(weights, V)\n",
    "\n",
    "\n",
    "attn(Q, K, V, mask=torch.triu(torch.ones((N, N), dtype=torch.bool), diagonal=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
